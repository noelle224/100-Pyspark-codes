# -*- coding: utf-8 -*-
"""6. Word count with dataframes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hIu-vTnzlrHZnQItey_HOKtW-pR8Y-l_
"""

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('df-word-count').getOrCreate()

"""- word count with dataframes, input : CSV"""

class Solution:
  def __init__(self):
    self.path = '/content/book.txt'

  def readcsv(self):
    df = spark.read.text(self.path)
    return df

  def splitting(self, df):
    from pyspark.sql.functions import split, col
    df = df.withColumn('word', split(col('value'), "\\W+"))
    df = df.select(df.word)
    return df

  def explode(self, df):
    from pyspark.sql.functions import explode, col, lower
    df = df.withColumn('words', explode(col('word')))
    ## cleaning the extra words
    df = df.filter(df.words != '')
    df = df.withColumn('smallwords', lower(col('words'))).select(col('smallwords'))
    return df

  def count_words(self, df):
    from pyspark.sql.functions import count
    df = df.groupBy('smallwords').agg(count('smallwords').alias('count')).sort('count', ascending=False)
    return df

  def most_frequent(self, df):
    most_frequent_row = df.first()
    if most_frequent_row:
        return most_frequent_row.smallwords  # This is a string
    else:
        return None  # This is also handled as a return


Ind_obj = Solution()
df = Ind_obj.readcsv()
df = Ind_obj.splitting(df)
df = Ind_obj.explode(df)
df = Ind_obj.count_words(df)
df.show()
word = Ind_obj.most_frequent(df)
print(word)

