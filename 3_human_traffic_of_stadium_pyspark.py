# -*- coding: utf-8 -*-
"""3. Human Traffic of stadium pyspark

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pWXP0lsP4ivcktzLXDOfFFRyHUfeuF26
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, DateType
from datetime import datetime

# Initialize Spark session
spark = SparkSession.builder.appName("StadiumTraffic").getOrCreate()

# Define schema
schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("visit_date", DateType(), True),
    StructField("no_of_people", IntegerType(), True)
])

# Create data
data = [
    (1, datetime.strptime('2017-07-01', '%Y-%m-%d'), 10),
    (2, datetime.strptime('2017-07-02', '%Y-%m-%d'), 103),
    (3, datetime.strptime('2017-07-03', '%Y-%m-%d'), 150),
    (4, datetime.strptime('2017-07-04', '%Y-%m-%d'), 99),
    (5, datetime.strptime('2017-07-05', '%Y-%m-%d'), 145),
    (6, datetime.strptime('2017-07-06', '%Y-%m-%d'), 1455),
    (7, datetime.strptime('2017-07-07', '%Y-%m-%d'), 199),
    (8, datetime.strptime('2017-07-08', '%Y-%m-%d'), 188)
]

# Create DataFrame
df = spark.createDataFrame(data, schema=schema)

# Show DataFrame
df.show()

from pyspark.sql.functions import col
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, date_add

def filter_data(df):
  windowspec = Window.orderBy(col('visit_date'))
  df = df.filter(col('no_of_people') >= 100)
  df = df.withColumn('row_number', row_number().over(windowspec))
  return df

def grouping_data(df):
  df = df.withColumn('grp_date', date_add(col('visit_date'), -col('row_number')))
  return df

def final_result(df):
  count_df = df.groupBy('grp_date').count().filter(col("count") >= 3)
  result_df = df.join(count_df, on="grp_date", how="inner").select("id", "visit_date", "no_of_people")
  result_df = result_df.orderBy(col("visit_date"))
  return result_df

df = filter_data(df)
df = grouping_data(df)
df = final_result(df)
df.show()